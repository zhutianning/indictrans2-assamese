#!/usr/bin/env python3
"""
åŸºäºå®˜æ–¹æŒ‡å—çš„ LoRA æ¨¡å‹è¯„ä¼°è„šæœ¬
ä½¿ç”¨æµ‹è¯•æ•°æ®è¿›è¡Œ BLEU å’Œ chrF è¯„åˆ†
"""

import os
import json
import torch
import pandas as pd
from datetime import datetime
from datasets import Dataset
from sacrebleu.metrics import BLEU, CHRF
from peft import PeftModel
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
)

# åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡
bleu_metric = BLEU()
chrf_metric = CHRF()

def load_data_from_official_format(data_dir, split="test"):
    """ä»å®˜æ–¹æ ¼å¼åŠ è½½æµ‹è¯•æ•°æ®"""
    print(f"ä» {data_dir} åŠ è½½ {split} æ•°æ®...")
    
    # æ„å»ºæ–‡ä»¶è·¯å¾„
    split_dir = os.path.join(data_dir, split, "eng_Latn-asm_Beng")
    src_file = os.path.join(split_dir, f"{split}.eng_Latn")
    tgt_file = os.path.join(split_dir, f"{split}.asm_Beng")
    
    if not os.path.exists(src_file) or not os.path.exists(tgt_file):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {src_file} æˆ– {tgt_file}")
    
    # è¯»å–æ•°æ®
    with open(src_file, 'r', encoding='utf-8') as f:
        src_lines = [line.strip() for line in f.readlines() if line.strip()]
    
    with open(tgt_file, 'r', encoding='utf-8') as f:
        tgt_lines = [line.strip() for line in f.readlines() if line.strip()]
    
    if len(src_lines) != len(tgt_lines):
        min_len = min(len(src_lines), len(tgt_lines))
        src_lines = src_lines[:min_len]
        tgt_lines = tgt_lines[:min_len]
        print(f"è­¦å‘Š: æºæ–‡ä»¶å’Œç›®æ ‡æ–‡ä»¶è¡Œæ•°ä¸åŒ¹é…ï¼Œä½¿ç”¨å‰ {min_len} è¡Œ")
    
    print(f"åŠ è½½äº† {len(src_lines)} ä¸ªæµ‹è¯•æ ·æœ¬")
    return src_lines, tgt_lines

def load_lora_model(model_path, base_model_name="ai4bharat/indictrans2-indic-en-dist-200M"):
    """åŠ è½½ LoRA å¾®è°ƒåçš„æ¨¡å‹"""
    print(f"åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_name}")
    
    # è®¾ç½®è®¤è¯
    from huggingface_hub import login
    login(token="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")
    
    # åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
    base_model = AutoModelForSeq2SeqLM.from_pretrained(
        base_model_name,
        trust_remote_code=True,
        attn_implementation="eager",
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
    )
    
    # åŠ è½½ LoRA é€‚é…å™¨
    print(f"åŠ è½½ LoRA é€‚é…å™¨: {model_path}")
    model = PeftModel.from_pretrained(base_model, model_path)
    
    # ç§»åŠ¨åˆ°è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device)
    model.eval()
    
    print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {device}")
    return model, tokenizer, device

def batch_translate_official(src_sentences, model, tokenizer, device, batch_size=4):
    """ä½¿ç”¨å®˜æ–¹æ–¹æ³•è¿›è¡Œæ‰¹é‡ç¿»è¯‘"""
    translations = []
    
    for i in range(0, len(src_sentences), batch_size):
        batch = src_sentences[i : i + batch_size]
        
        # ä½¿ç”¨å®˜æ–¹æ ¼å¼ï¼šsrc_lang tgt_lang text
        formatted_batch = [f"eng_Latn asm_Beng {text}" for text in batch]
        
        # åˆ†è¯
        inputs = tokenizer(
            formatted_batch,
            truncation=True,
            padding="longest",
            return_tensors="pt",
            return_attention_mask=True,
        ).to(device)
        
        # ç”Ÿæˆç¿»è¯‘
        with torch.no_grad():
            generated_tokens = model.generate(
                **inputs,
                use_cache=True,
                min_length=0,
                max_length=256,
                num_beams=5,
                num_return_sequences=1,
                early_stopping=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # è§£ç 
        batch_translations = tokenizer.batch_decode(
            generated_tokens,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=True,
        )
        
        translations.extend(batch_translations)
        
        # æ¸…ç†å†…å­˜
        del inputs
        if device == "cuda":
            torch.cuda.empty_cache()
    
    return translations

def compute_metrics_official(predictions, references, metric_dict=None):
    """ä½¿ç”¨å®˜æ–¹æ–¹æ³•è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    if metric_dict is None:
        metric_dict = {"BLEU": bleu_metric, "chrF": chrf_metric}
    
    results = {}
    for metric_name, metric in metric_dict.items():
        try:
            score = metric.corpus_score(predictions, [references]).score
            results[metric_name] = score
            print(f"{metric_name}: {score:.4f}")
        except Exception as e:
            print(f"è®¡ç®— {metric_name} æ—¶å‡ºé”™: {e}")
            results[metric_name] = 0.0
    
    return results

def evaluate_lora_model():
    """è¯„ä¼° LoRA å¾®è°ƒåçš„æ¨¡å‹"""
    
    print("=== åŸºäºå®˜æ–¹æŒ‡å—çš„ LoRA æ¨¡å‹è¯„ä¼° ===")
    
    # æ£€æŸ¥è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    if device == "cuda":
        print(f"GPU: {torch.cuda.get_device_name(0)}")
    
    # æ•°æ®è·¯å¾„
    data_dir = "assamese_english_official_format"
    
    # æ‰¾åˆ°æœ€æ–°çš„æ¨¡å‹è·¯å¾„
    outputs_dir = "outputs"
    model_dirs = [d for d in os.listdir(outputs_dir) if d.startswith("assamese_english_lora_cuda_fixed_")]
    if not model_dirs:
        print("é”™è¯¯: æ‰¾ä¸åˆ°è®­ç»ƒå¥½çš„ LoRA æ¨¡å‹")
        return
    
    latest_model_dir = sorted(model_dirs)[-1]
    model_path = os.path.join(outputs_dir, latest_model_dir)
    print(f"ä½¿ç”¨æ¨¡å‹: {model_path}")
    
    try:
        # åŠ è½½æ¨¡å‹
        model, tokenizer, device = load_lora_model(model_path)
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        print("\n1. åŠ è½½æµ‹è¯•æ•°æ®...")
        src_sentences, tgt_sentences = load_data_from_official_format(data_dir, "test")
        
        # å¦‚æœæµ‹è¯•æ•°æ®å¤ªå¤šï¼Œä½¿ç”¨å‰100ä¸ªæ ·æœ¬è¿›è¡Œå¿«é€Ÿè¯„ä¼°
        if len(src_sentences) > 100:
            print(f"æµ‹è¯•æ•°æ®è¾ƒå¤š({len(src_sentences)}ä¸ª)ï¼Œä½¿ç”¨å‰100ä¸ªæ ·æœ¬è¿›è¡Œå¿«é€Ÿè¯„ä¼°")
            src_sentences = src_sentences[:100]
            tgt_sentences = tgt_sentences[:100]
        
        print(f"è¯„ä¼°æ ·æœ¬æ•°: {len(src_sentences)}")
        
        # ç”Ÿæˆç¿»è¯‘
        print("\n2. ç”Ÿæˆç¿»è¯‘...")
        predictions = batch_translate_official(src_sentences, model, tokenizer, device)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        print("\n3. è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        metrics = compute_metrics_official(predictions, tgt_sentences)
        
        # æ˜¾ç¤ºæ ·æœ¬ç»“æœ
        print("\n4. æ ·æœ¬ç»“æœ:")
        n_samples = min(5, len(src_sentences))
        for i in range(n_samples):
            print(f"\næ ·æœ¬ {i+1}:")
            print(f"è‹±è¯­: {src_sentences[i]}")
            print(f"å‚è€ƒç¿»è¯‘: {tgt_sentences[i]}")
            print(f"æ¨¡å‹ç¿»è¯‘: {predictions[i]}")
            print("-" * 50)
        
        # ä¿å­˜è¯„ä¼°ç»“æœ
        results = {
            'timestamp': datetime.now().isoformat(),
            'model_path': model_path,
            'test_samples': len(src_sentences),
            'metrics': metrics,
            'samples': [
                {
                    'source': src_sentences[i],
                    'reference': tgt_sentences[i],
                    'prediction': predictions[i]
                }
                for i in range(min(10, len(src_sentences)))
            ]
        }
        
        results_file = f"{model_path}/evaluation_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ“ è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        print(f"\n=== æœ€ç»ˆè¯„ä¼°ç»“æœ ===")
        for metric_name, score in metrics.items():
            print(f"{metric_name}: {score:.4f}")
        
        return results
        
    except Exception as e:
        print(f"âœ— è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹åŸºäºå®˜æ–¹æŒ‡å—çš„ LoRA æ¨¡å‹è¯„ä¼°...")
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
    data_dir = "assamese_english_official_format"
    if not os.path.exists(data_dir):
        print(f"é”™è¯¯: æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        print("è¯·å…ˆè¿è¡Œ prepare_data_for_official_lora.py å‡†å¤‡æ•°æ®")
        return
    
    # æ‰§è¡Œè¯„ä¼°
    results = evaluate_lora_model()
    
    if results:
        print("\nğŸ‰ LoRA æ¨¡å‹è¯„ä¼°å®Œæˆï¼")
    else:
        print("\nâŒ è¯„ä¼°å¤±è´¥")

if __name__ == "__main__":
    main()
