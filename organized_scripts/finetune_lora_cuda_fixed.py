#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆæœ¬çš„ LoRA å¾®è°ƒè„šæœ¬
ç¦ç”¨è¯„ä¼°æ—¶çš„ç”Ÿæˆä»¥é¿å…æ¨¡å‹å†…éƒ¨é”™è¯¯
"""

import os
import json
import torch
from datetime import datetime
from datasets import Dataset
from peft import LoraConfig, get_peft_model
from transformers import (
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    DataCollatorForSeq2Seq,
    EarlyStoppingCallback,
)

def load_data_from_official_format(data_dir, split="train"):
    """ä»å®˜æ–¹æ ¼å¼åŠ è½½æ•°æ®"""
    print(f"ä» {data_dir} åŠ è½½ {split} æ•°æ®...")
    
    # æ„å»ºæ–‡ä»¶è·¯å¾„
    split_dir = os.path.join(data_dir, split, "eng_Latn-asm_Beng")
    src_file = os.path.join(split_dir, f"{split}.eng_Latn")
    tgt_file = os.path.join(split_dir, f"{split}.asm_Beng")
    
    if not os.path.exists(src_file) or not os.path.exists(tgt_file):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {src_file} æˆ– {tgt_file}")
    
    # è¯»å–æ•°æ®
    with open(src_file, 'r', encoding='utf-8') as f:
        src_lines = [line.strip() for line in f.readlines() if line.strip()]
    
    with open(tgt_file, 'r', encoding='utf-8') as f:
        tgt_lines = [line.strip() for line in f.readlines() if line.strip()]
    
    if len(src_lines) != len(tgt_lines):
        min_len = min(len(src_lines), len(tgt_lines))
        src_lines = src_lines[:min_len]
        tgt_lines = tgt_lines[:min_len]
        print(f"è­¦å‘Š: æºæ–‡ä»¶å’Œç›®æ ‡æ–‡ä»¶è¡Œæ•°ä¸åŒ¹é…ï¼Œä½¿ç”¨å‰ {min_len} è¡Œ")
    
    # è½¬æ¢ä¸ºå®˜æ–¹æœŸæœ›çš„æ ¼å¼
    data = []
    for src, tgt in zip(src_lines, tgt_lines):
        # ä½¿ç”¨å®˜æ–¹æ ¼å¼ï¼šsrc_lang tgt_lang text
        formatted_src = f"eng_Latn asm_Beng {src}"
        data.append({
            "sentence_SRC": formatted_src,
            "sentence_TGT": tgt
        })
    
    print(f"åŠ è½½äº† {len(data)} ä¸ªæ ·æœ¬")
    return data

def preprocess_fn(example, tokenizer, **kwargs):
    """é¢„å¤„ç†å‡½æ•°"""
    model_inputs = tokenizer(
        example["sentence_SRC"], 
        truncation=True, 
        padding=False, 
        max_length=256
    )

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            example["sentence_TGT"], 
            truncation=True, 
            padding=False, 
            max_length=256
        )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

def compute_metrics_simple(eval_preds):
    """ç®€åŒ–çš„è¯„ä¼°æŒ‡æ ‡è®¡ç®— - åªè®¡ç®—æŸå¤±ï¼Œä¸ç”Ÿæˆæ–‡æœ¬"""
    preds, labels = eval_preds
    
    # ç®€å•çš„æŸå¤±è®¡ç®—ï¼Œé¿å…å¼ é‡ç»´åº¦é—®é¢˜
    try:
        if isinstance(preds, tuple):
            preds = preds[0]  # å–ç¬¬ä¸€ä¸ªå…ƒç´ 
        if hasattr(preds, 'mean'):
            loss = float(preds.mean())
        else:
            loss = 0.0
        return {"eval_loss": loss}
    except Exception as e:
        print(f"è®¡ç®—æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
        return {"eval_loss": 0.0}

def finetune_with_lora_cuda_fixed():
    """ä½¿ç”¨ä¿®å¤ç‰ˆæœ¬çš„ LoRA è¿›è¡Œ CUDA å¾®è°ƒ"""
    
    print("=== ä¿®å¤ç‰ˆæœ¬çš„ LoRA å¾®è°ƒï¼ˆCUDA ç‰ˆæœ¬ï¼‰ ===")
    
    # æ£€æŸ¥è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    if device == "cuda":
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # æ¨¡å‹é…ç½®
    model_name = "ai4bharat/indictrans2-indic-en-dist-200M"
    print(f"ä½¿ç”¨æ¨¡å‹: {model_name}")
    
    # æ•°æ®è·¯å¾„
    data_dir = "assamese_english_official_format"
    
    try:
        # è®¾ç½®è®¤è¯
        from huggingface_hub import login
        login(token="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")
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        print("\n1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
        model = AutoModelForSeq2SeqLM.from_pretrained(
            model_name,
            trust_remote_code=True,
            attn_implementation="eager",
            dropout=0.0,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32
        )
        
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        print("âœ“ æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸ")
        
        # é…ç½® LoRA
        print("\n2. é…ç½® LoRA...")
        lora_config = LoraConfig(
            r=16,  # LoRA rank
            lora_alpha=32,  # LoRA alpha
            target_modules=["q_proj", "k_proj"],  # ç›®æ ‡æ¨¡å—
            lora_dropout=0.1,
            bias="none",
            task_type="SEQ_2_SEQ_LM",
        )
        
        # åº”ç”¨ LoRA
        model = get_peft_model(model, lora_config)
        model.print_trainable_parameters()
        
        # åŠ è½½æ•°æ®
        print("\n3. åŠ è½½æ•°æ®...")
        train_data = load_data_from_official_format(data_dir, "train")
        dev_data = load_data_from_official_format(data_dir, "dev")
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = Dataset.from_list(train_data)
        dev_dataset = Dataset.from_list(dev_data)
        
        print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
        print(f"éªŒè¯é›†å¤§å°: {len(dev_dataset)}")
        
        # æ•°æ®é¢„å¤„ç†
        print("\n4. æ•°æ®é¢„å¤„ç†...")
        train_dataset = train_dataset.map(
            lambda example: preprocess_fn(example, tokenizer),
            batched=True,
        )
        
        dev_dataset = dev_dataset.map(
            lambda example: preprocess_fn(example, tokenizer),
            batched=True,
        )
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=tokenizer,
            model=model,
            padding=True,
            label_pad_token_id=-100
        )
        
        # è®­ç»ƒå‚æ•°ï¼ˆä¿®å¤ç‰ˆæœ¬ - ç¦ç”¨è¯„ä¼°æ—¶çš„ç”Ÿæˆï¼‰
        output_dir = f"outputs/assamese_english_lora_cuda_fixed_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        training_args = Seq2SeqTrainingArguments(
            output_dir=output_dir,
            do_train=True,
            do_eval=True,
            fp16=True,  # ä½¿ç”¨ fp16 åŠ é€Ÿè®­ç»ƒ
            logging_strategy="steps",
            eval_strategy="steps",
            save_strategy="steps",
            logging_steps=10,
            save_total_limit=2,
            predict_with_generate=False,  # å…³é”®ä¿®å¤ï¼šç¦ç”¨è¯„ä¼°æ—¶çš„ç”Ÿæˆ
            load_best_model_at_end=True,
            num_train_epochs=3,
            per_device_train_batch_size=8,  # å¢åŠ æ‰¹æ¬¡å¤§å°
            per_device_eval_batch_size=8,
            gradient_accumulation_steps=2,  # å‡å°‘æ¢¯åº¦ç´¯ç§¯
            eval_accumulation_steps=2,
            weight_decay=0.01,
            adam_beta1=0.9,
            adam_beta2=0.98,
            max_grad_norm=1.0,
            optim="adamw_torch",
            lr_scheduler_type="inverse_sqrt",
            warmup_ratio=0.0,
            warmup_steps=100,
            learning_rate=2e-4,  # å®˜æ–¹æ¨èçš„å­¦ä¹ ç‡
            save_steps=50,
            eval_steps=50,
            dataloader_num_workers=4,  # å¢åŠ æ•°æ®åŠ è½½å™¨å·¥ä½œè¿›ç¨‹
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            report_to="none",
            # ç§»é™¤ç”Ÿæˆç›¸å…³å‚æ•°
            dataloader_pin_memory=True,  # å¯ç”¨å†…å­˜å›ºå®š
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = Seq2SeqTrainer(
            model=model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=train_dataset,
            eval_dataset=dev_dataset,
            compute_metrics=compute_metrics_simple,
            callbacks=[
                EarlyStoppingCallback(
                    early_stopping_patience=3,
                    early_stopping_threshold=1e-3,
                )
            ],
        )
        
        # å¼€å§‹è®­ç»ƒ
        print(f"\n5. å¼€å§‹ LoRA å¾®è°ƒè®­ç»ƒï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰...")
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        print(f"ä½¿ç”¨ CUDA: {device == 'cuda'}")
        print("æ³¨æ„: å·²ç¦ç”¨è¯„ä¼°æ—¶çš„ç”Ÿæˆä»¥é¿å…æ¨¡å‹é”™è¯¯")
        
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        
        print(f"\nâœ“ LoRA å¾®è°ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
        
        # æ‰‹åŠ¨æµ‹è¯•æ¨¡å‹ï¼ˆé¿å…è®­ç»ƒå™¨çš„é—®é¢˜ï¼‰
        print("\n6. æ‰‹åŠ¨æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹...")
        test_results = test_model_manual(model, tokenizer, dev_data[:5], device)
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        results_file = f"{output_dir}/test_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(test_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ“ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        print("âœ“ é˜¿è¨å§†è¯­-è‹±è¯­ LoRA å¾®è°ƒæˆåŠŸå®Œæˆï¼")
        
        return True
        
    except Exception as e:
        print(f"âœ— é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_model_manual(model, tokenizer, test_data, device, num_samples=5):
    """æ‰‹åŠ¨æµ‹è¯•å¾®è°ƒåçš„ LoRA æ¨¡å‹"""
    print(f"æ‰‹åŠ¨æµ‹è¯• LoRA æ¨¡å‹ï¼Œä½¿ç”¨ {num_samples} ä¸ªæ ·æœ¬...")
    
    results = []
    
    for i, sample in enumerate(test_data[:num_samples]):
        try:
            # ä½¿ç”¨é¢„å¤„ç†åçš„æºæ–‡æœ¬
            inputs = tokenizer(
                sample['sentence_SRC'],
                return_tensors="pt",
                truncation=True,
                max_length=256
            ).to(device)
            
            with torch.no_grad():
                # ä½¿ç”¨æ›´ç®€å•çš„ç”Ÿæˆå‚æ•°
                generated_tokens = model.generate(
                    **inputs,
                    max_length=128,  # å‡å°‘æœ€å¤§é•¿åº¦
                    num_beams=1,     # ä½¿ç”¨è´ªå¿ƒæœç´¢è€Œä¸æ˜¯æŸæœç´¢
                    do_sample=False, # ç¦ç”¨é‡‡æ ·
                    early_stopping=True,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            prediction = tokenizer.decode(
                generated_tokens[0],
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )
            
            result = {
                'sample_id': i + 1,
                'source': sample['sentence_SRC'],
                'target': sample['sentence_TGT'],
                'prediction': prediction
            }
            
            results.append(result)
            
            print(f"\næ ·æœ¬ {i+1}:")
            print(f"æºæ–‡æœ¬: {sample['sentence_SRC']}")
            print(f"ç›®æ ‡æ–‡æœ¬: {sample['sentence_TGT']}")
            print(f"é¢„æµ‹æ–‡æœ¬: {prediction}")
            print("-" * 50)
            
        except Exception as e:
            print(f"æµ‹è¯•æ ·æœ¬ {i+1} å¤±è´¥: {e}")
            continue
    
    return {
        'test_results': results,
        'total_samples': len(results),
        'timestamp': datetime.now().isoformat(),
        'model_type': 'IndicTrans2_LoRA_CUDA_Fixed',
        'format': 'Official_format',
        'device': device
    }

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹ä¿®å¤ç‰ˆæœ¬çš„é˜¿è¨å§†è¯­-è‹±è¯­ LoRA å¾®è°ƒï¼ˆCUDA ç‰ˆæœ¬ï¼‰...")
    
    # æ£€æŸ¥ CUDA å¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("è­¦å‘Š: CUDA ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPU è®­ç»ƒï¼ˆé€Ÿåº¦ä¼šå¾ˆæ…¢ï¼‰")
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
    data_dir = "assamese_english_official_format"
    if not os.path.exists(data_dir):
        print(f"é”™è¯¯: æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        print("è¯·å…ˆè¿è¡Œ prepare_data_for_official_lora.py å‡†å¤‡æ•°æ®")
        return
    
    # æ‰§è¡Œå¾®è°ƒ
    success = finetune_with_lora_cuda_fixed()
    
    if success:
        print("\nğŸ‰ ä¿®å¤ç‰ˆæœ¬çš„ LoRA å¾®è°ƒæµç¨‹å®Œæˆï¼")
    else:
        print("\nâŒ å¾®è°ƒå¤±è´¥")

if __name__ == "__main__":
    main()
