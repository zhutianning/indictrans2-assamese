#!/usr/bin/env python3
"""
ç®€åŒ–çš„ LoRA æ¨¡å‹è¯„ä¼°è„šæœ¬
ä½¿ç”¨æ›´ç®€å•çš„ç”Ÿæˆå‚æ•°é¿å…æ¨¡å‹é”™è¯¯
"""

import os
import json
import torch
import pandas as pd
from datetime import datetime
from sacrebleu.metrics import BLEU, CHRF
from peft import PeftModel
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
)

# åˆå§‹åŒ–è¯„ä¼°æŒ‡æ ‡
bleu_metric = BLEU()
chrf_metric = CHRF()

def load_test_data():
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    print("åŠ è½½æµ‹è¯•æ•°æ®...")
    
    data_dir = "assamese_english_official_format"
    test_dir = os.path.join(data_dir, "test", "eng_Latn-asm_Beng")
    src_file = os.path.join(test_dir, "test.eng_Latn")
    tgt_file = os.path.join(test_dir, "test.asm_Beng")
    
    with open(src_file, 'r', encoding='utf-8') as f:
        src_sentences = [line.strip() for line in f.readlines() if line.strip()]
    
    with open(tgt_file, 'r', encoding='utf-8') as f:
        tgt_sentences = [line.strip() for line in f.readlines() if line.strip()]
    
    print(f"åŠ è½½äº† {len(src_sentences)} ä¸ªæµ‹è¯•æ ·æœ¬")
    return src_sentences, tgt_sentences

def load_lora_model(model_path):
    """åŠ è½½ LoRA å¾®è°ƒåçš„æ¨¡å‹"""
    print(f"åŠ è½½ LoRA æ¨¡å‹: {model_path}")
    
    # è®¾ç½®è®¤è¯
    from huggingface_hub import login
    login(token="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")
    
    # åŠ è½½åŸºç¡€æ¨¡å‹å’Œåˆ†è¯å™¨
    base_model_name = "ai4bharat/indictrans2-indic-en-dist-200M"
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
    base_model = AutoModelForSeq2SeqLM.from_pretrained(
        base_model_name,
        trust_remote_code=True,
        attn_implementation="eager",
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
    )
    
    # åŠ è½½ LoRA é€‚é…å™¨
    model = PeftModel.from_pretrained(base_model, model_path)
    
    # ç§»åŠ¨åˆ°è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = model.to(device)
    model.eval()
    
    print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {device}")
    return model, tokenizer, device

def simple_translate(src_sentences, model, tokenizer, device, max_samples=20):
    """ä½¿ç”¨ç®€å•å‚æ•°è¿›è¡Œç¿»è¯‘"""
    print(f"å¼€å§‹ç¿»è¯‘ï¼Œæœ€å¤šå¤„ç† {max_samples} ä¸ªæ ·æœ¬...")
    
    predictions = []
    successful = 0
    failed = 0
    
    for i, src_text in enumerate(src_sentences[:max_samples]):
        try:
            # ä½¿ç”¨å®˜æ–¹æ ¼å¼ï¼šsrc_lang tgt_lang text
            formatted_input = f"eng_Latn asm_Beng {src_text}"
            
            # åˆ†è¯
            inputs = tokenizer(
                formatted_input,
                return_tensors="pt",
                truncation=True,
                max_length=128  # å‡å°‘æœ€å¤§é•¿åº¦
            ).to(device)
            
            # ä½¿ç”¨æœ€ç®€å•çš„ç”Ÿæˆå‚æ•°
            with torch.no_grad():
                generated_tokens = model.generate(
                    **inputs,
                    max_length=64,  # è¿›ä¸€æ­¥å‡å°‘æœ€å¤§é•¿åº¦
                    num_beams=1,    # ä½¿ç”¨è´ªå¿ƒæœç´¢
                    do_sample=False, # ç¦ç”¨é‡‡æ ·
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    early_stopping=True
                )
            
            # è§£ç 
            prediction = tokenizer.decode(
                generated_tokens[0],
                skip_special_tokens=True,
                clean_up_tokenization_spaces=True
            )
            
            predictions.append(prediction)
            successful += 1
            
            if (i + 1) % 5 == 0:
                print(f"å·²å¤„ç† {i + 1}/{max_samples} ä¸ªæ ·æœ¬")
            
        except Exception as e:
            print(f"ç¿»è¯‘æ ·æœ¬ {i+1} å¤±è´¥: {e}")
            predictions.append("")  # æ·»åŠ ç©ºå­—ç¬¦ä¸²ä¿æŒç´¢å¼•ä¸€è‡´
            failed += 1
            continue
    
    print(f"ç¿»è¯‘å®Œæˆ: {successful} æˆåŠŸ, {failed} å¤±è´¥")
    return predictions

def compute_metrics(predictions, references):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    print("è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    
    # è¿‡æ»¤æ‰ç©ºçš„é¢„æµ‹
    valid_pairs = [(pred, ref) for pred, ref in zip(predictions, references) if pred.strip()]
    if not valid_pairs:
        print("æ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ç»“æœ")
        return {"BLEU": 0.0, "chrF": 0.0}
    
    valid_predictions, valid_references = zip(*valid_pairs)
    
    try:
        bleu_score = bleu_metric.corpus_score(valid_predictions, [valid_references]).score
        chrf_score = chrf_metric.corpus_score(valid_predictions, [valid_references]).score
        
        return {"BLEU": bleu_score, "chrF": chrf_score}
    except Exception as e:
        print(f"è®¡ç®—æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
        return {"BLEU": 0.0, "chrF": 0.0}

def evaluate_simple():
    """ç®€åŒ–çš„è¯„ä¼°æµç¨‹"""
    print("=== ç®€åŒ–çš„ LoRA æ¨¡å‹è¯„ä¼° ===")
    
    # æ£€æŸ¥è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ‰¾åˆ°æœ€æ–°çš„æ¨¡å‹
    outputs_dir = "outputs"
    model_dirs = [d for d in os.listdir(outputs_dir) if d.startswith("assamese_english_lora_cuda_fixed_")]
    if not model_dirs:
        print("é”™è¯¯: æ‰¾ä¸åˆ°è®­ç»ƒå¥½çš„ LoRA æ¨¡å‹")
        return
    
    latest_model_dir = sorted(model_dirs)[-1]
    model_path = os.path.join(outputs_dir, latest_model_dir)
    print(f"ä½¿ç”¨æ¨¡å‹: {model_path}")
    
    try:
        # åŠ è½½æ¨¡å‹å’Œæ•°æ®
        model, tokenizer, device = load_lora_model(model_path)
        src_sentences, tgt_sentences = load_test_data()
        
        # ç”Ÿæˆç¿»è¯‘
        print("\nå¼€å§‹ç”Ÿæˆç¿»è¯‘...")
        predictions = simple_translate(src_sentences, model, tokenizer, device, max_samples=20)
        
        # è®¡ç®—æŒ‡æ ‡
        print("\nè®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        metrics = compute_metrics(predictions, tgt_sentences[:len(predictions)])
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\n=== è¯„ä¼°ç»“æœ ===")
        print(f"BLEU: {metrics['BLEU']:.4f}")
        print(f"chrF: {metrics['chrF']:.4f}")
        
        # æ˜¾ç¤ºæ ·æœ¬
        print(f"\n=== æ ·æœ¬ç»“æœ ===")
        n_samples = min(5, len(predictions))
        for i in range(n_samples):
            if predictions[i].strip():  # åªæ˜¾ç¤ºæˆåŠŸçš„ç¿»è¯‘
                print(f"\næ ·æœ¬ {i+1}:")
                print(f"è‹±è¯­: {src_sentences[i]}")
                print(f"å‚è€ƒç¿»è¯‘: {tgt_sentences[i]}")
                print(f"æ¨¡å‹ç¿»è¯‘: {predictions[i]}")
                print("-" * 50)
        
        # ä¿å­˜ç»“æœ
        results = {
            'timestamp': datetime.now().isoformat(),
            'model_path': model_path,
            'test_samples': len(predictions),
            'metrics': metrics,
            'samples': [
                {
                    'source': src_sentences[i],
                    'reference': tgt_sentences[i],
                    'prediction': predictions[i]
                }
                for i in range(min(10, len(predictions)))
            ]
        }
        
        results_file = f"{model_path}/simple_evaluation_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ“ è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        return results
        
    except Exception as e:
        print(f"âœ— è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹ç®€åŒ–çš„ LoRA æ¨¡å‹è¯„ä¼°...")
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
    data_dir = "assamese_english_official_format"
    if not os.path.exists(data_dir):
        print(f"é”™è¯¯: æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # æ‰§è¡Œè¯„ä¼°
    results = evaluate_simple()
    
    if results:
        print("\nğŸ‰ ç®€åŒ–è¯„ä¼°å®Œæˆï¼")
    else:
        print("\nâŒ è¯„ä¼°å¤±è´¥")

if __name__ == "__main__":
    main()
