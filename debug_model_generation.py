#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è°ƒè¯•æ¨¡å‹ç”Ÿæˆé—®é¢˜çš„è„šæœ¬
å°è¯•ä¸åŒçš„æ–¹æ³•æ¥ä¿®å¤ AttributeError: 'NoneType' object has no attribute 'shape'
"""

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from huggingface_hub import login
import traceback

def setup_auth():
    """è®¾ç½® Hugging Face è®¤è¯"""
    try:
        login(token="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")
        print("Hugging Face è®¤è¯æˆåŠŸ")
        return True
    except Exception as e:
        print(f"è®¤è¯å¤±è´¥: {e}")
        return False

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    try:
        print("ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹...")
        model_name = "ai4bharat/indictrans2-indic-en-dist-200M"
        
        # åŠ è½½ tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        print("âœ… Tokenizer åŠ è½½æˆåŠŸ")
        
        # åŠ è½½æ¨¡å‹
        model = AutoModelForSeq2SeqLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        return model, tokenizer
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        traceback.print_exc()
        return None, None

def test_simple_generation(model, tokenizer):
    """æµ‹è¯•ç®€å•ç”Ÿæˆ"""
    try:
        print("ğŸ”„ æµ‹è¯•ç®€å•ç”Ÿæˆ...")
        
        # å‡†å¤‡è¾“å…¥
        text = "Hello world"
        inputs = tokenizer(text, return_tensors="pt")
        
        # ç§»åŠ¨åˆ° GPU
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
            model = model.cuda()
        
        print(f"è¾“å…¥ tokens: {inputs['input_ids'][0][:10].tolist()}")
        print(f"è¾“å…¥ shape: {inputs['input_ids'].shape}")
        
        # å°è¯•ç”Ÿæˆ
        with torch.no_grad():
            outputs = model.generate(
                inputs['input_ids'],
                max_length=32,
                num_beams=1,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        result = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"âœ… ç”ŸæˆæˆåŠŸ: {result}")
        return True
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_indic_format(model, tokenizer):
    """æµ‹è¯• IndicTrans2 æ ¼å¼"""
    try:
        print("ğŸ”„ æµ‹è¯• IndicTrans2 æ ¼å¼...")
        
        # ä½¿ç”¨æ­£ç¡®çš„æ ¼å¼
        text = "asm_Beng eng_Latn à¦®à¦‡ à¦­à¦¾à¦² à¦†à¦›à§‹"
        inputs = tokenizer(text, return_tensors="pt")
        
        # ç§»åŠ¨åˆ° GPU
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
            model = model.cuda()
        
        print(f"è¾“å…¥ tokens: {inputs['input_ids'][0][:10].tolist()}")
        print(f"è¾“å…¥ shape: {inputs['input_ids'].shape}")
        
        # å°è¯•ç”Ÿæˆ
        with torch.no_grad():
            outputs = model.generate(
                inputs['input_ids'],
                max_length=64,
                num_beams=1,
                do_sample=False,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        result = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"âœ… IndicTrans2 æ ¼å¼ç”ŸæˆæˆåŠŸ: {result}")
        return True
        
    except Exception as e:
        print(f"âŒ IndicTrans2 æ ¼å¼ç”Ÿæˆå¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_different_strategies(model, tokenizer):
    """æµ‹è¯•ä¸åŒçš„ç”Ÿæˆç­–ç•¥"""
    strategies = [
        {"name": "Greedy", "max_length": 32, "num_beams": 1, "do_sample": False},
        {"name": "Beam Search", "max_length": 32, "num_beams": 3, "do_sample": False},
        {"name": "Sampling", "max_length": 32, "do_sample": True, "temperature": 0.7},
        {"name": "Top-k", "max_length": 32, "do_sample": True, "top_k": 50},
        {"name": "Top-p", "max_length": 32, "do_sample": True, "top_p": 0.9},
    ]
    
    text = "asm_Beng eng_Latn à¦®à¦‡ à¦­à¦¾à¦² à¦†à¦›à§‹"
    inputs = tokenizer(text, return_tensors="pt")
    
    if torch.cuda.is_available():
        inputs = {k: v.cuda() for k, v in inputs.items()}
        model = model.cuda()
    
    for strategy in strategies:
        try:
            print(f"ğŸ”„ æµ‹è¯•ç­–ç•¥: {strategy['name']}")
            
            with torch.no_grad():
                outputs = model.generate(
                    inputs['input_ids'],
                    **{k: v for k, v in strategy.items() if k != 'name'},
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            result = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"âœ… {strategy['name']} æˆåŠŸ: {result}")
            
        except Exception as e:
            print(f"âŒ {strategy['name']} å¤±è´¥: {e}")

def test_model_internals(model, tokenizer):
    """æµ‹è¯•æ¨¡å‹å†…éƒ¨çŠ¶æ€"""
    try:
        print("ğŸ”„ æ£€æŸ¥æ¨¡å‹å†…éƒ¨çŠ¶æ€...")
        
        text = "asm_Beng eng_Latn à¦®à¦‡ à¦­à¦¾à¦² à¦†à¦›à§‹"
        inputs = tokenizer(text, return_tensors="pt")
        
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
            model = model.cuda()
        
        # æ£€æŸ¥æ¨¡å‹çŠ¶æ€
        print(f"æ¨¡å‹è®¾å¤‡: {model.device}")
        print(f"æ¨¡å‹æ•°æ®ç±»å‹: {model.dtype}")
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
        
        # å°è¯•å‰å‘ä¼ æ’­
        with torch.no_grad():
            outputs = model(**inputs)
            print(f"å‰å‘ä¼ æ’­æˆåŠŸ")
            print(f"è¾“å‡º keys: {outputs.keys()}")
            if hasattr(outputs, 'logits'):
                print(f"Logits shape: {outputs.logits.shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹å†…éƒ¨æ£€æŸ¥å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹è°ƒè¯•æ¨¡å‹ç”Ÿæˆé—®é¢˜")
    print("=" * 50)
    
    # 1. è®¾ç½®è®¤è¯
    if not setup_auth():
        return
    
    # 2. åŠ è½½æ¨¡å‹
    model, tokenizer = test_model_loading()
    if model is None or tokenizer is None:
        return
    
    # 3. æ£€æŸ¥æ¨¡å‹å†…éƒ¨çŠ¶æ€
    test_model_internals(model, tokenizer)
    
    # 4. æµ‹è¯•ç®€å•ç”Ÿæˆ
    test_simple_generation(model, tokenizer)
    
    # 5. æµ‹è¯• IndicTrans2 æ ¼å¼
    test_indic_format(model, tokenizer)
    
    # 6. æµ‹è¯•ä¸åŒç­–ç•¥
    test_different_strategies(model, tokenizer)
    
    print("=" * 50)
    print("ğŸ¯ è°ƒè¯•å®Œæˆ")

if __name__ == "__main__":
    main()
